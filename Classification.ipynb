{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5TOYdfpOkz0ofd0HWjqBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muntazirabidi/CLASS_CAMB/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST Dataset\n",
        "\n",
        "MNIST (Modified National Institute of Standards and Technology) is a dataset of images of handwritten digits, originally developed by Yann LeCun and his team at the Department of Computer Science and Electrical Engineering at the University of Maryland. The dataset is often used to train and evaluate machine learning models for image classification tasks. It consists of a training set of 60,000 images and a test set of 10,000 images, all of which are 28x28 grayscale images of handwritten digits from 0 to 9. The images are pre-processed and normalized, and the task is to classify each image into one of the 10 digit classes. MNIST is a widely used dataset in the field of machine learning and is considered a \"Hello, World!\" example for image classification algorithms.\n",
        "\n",
        "scikit-Learn provides manmy helper functions to download popular datasets. MNIST is one of them. \n"
      ],
      "metadata": {
        "id": "CzgJYeb8wg-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "CgYbqU3ZkrNU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJzxbtbPcJlj",
        "outputId": "cb03bda0-5d40-4459-c8ed-e85f82360176"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X, y = mnist['data'], mnist['target']\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCGrR6RKcgNf",
        "outputId": "e861d38c-0a6d-4b2a-81d2-f3733273d3b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import set_matplotlib_formats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt \n",
        "some_digit = X.to_numpy()[0]\n",
        "some_digit_image = some_digit.reshape(28,28)\n",
        "plt.imshow(some_digit_image,cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "hTx6nuJBc8Ie",
        "outputId": "117ef3f1-92b2-4450-8466-673048e5e608"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=y.astype(np.uint8)"
      ],
      "metadata": {
        "id": "mr2NiwkBdtNW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = X.to_numpy()[:60000], X.to_numpy()[60000:], y.to_numpy()[:60000], y.to_numpy()[60000:]"
      ],
      "metadata": {
        "id": "Iv8_8WMedcFE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Binary Classifier \n",
        "\n",
        "A binary classifier is a machine learning model that makes predictions about a binary outcome. The outcome is either one of two possible classes, labeled as positive or negative. For example, a binary classifier might be used to predict whether an email is spam (positive class) or not spam (negative class). The classifier would be trained on a dataset of labeled emails and would learn to predict the class of an email based on features of the email such as the subject line, the sender, and the content of the message. When given a new email, the classifier would output a prediction of whether the email is spam or not.\n",
        "\n",
        "Lets only try to identify one digit, lets say 5. This is an example of a binary classifier, distinguishing between just two classes. "
      ],
      "metadata": {
        "id": "cVZA3qyPw3YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Classifier\n",
        "y_train_5 = (y_train == 5)\n",
        "y_test_5 = (y_test == 5)"
      ],
      "metadata": {
        "id": "b5mHQQuLiwSF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcNwiqSZmz4Y",
        "outputId": "1120fa7d-6e84-449a-a554-3340d4fa3cf3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_clf.predict([some_digit])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-9ZhuG-nItn",
        "outputId": "9a94ebe1-6da2-4cdc-9eb2-78be4a38e4ee"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring Accuracy Using Cross-Validation\n",
        "\n",
        "A good way to evaluate a model is to use the cross-validation.\n",
        "\n",
        "*'StratifiedKFold*' is a K-fold cross-validation iterator that returns stratified folds. Stratified folds means that the proportions of the different classes in the folds are the same as the proportions of the different classes in the entire dataset. This is particularly useful for datasets where the distribution of classes is unbalanced, as it ensures that each fold has a representative ratio of classes. This can be useful in cases where the model's performance depends on the relative proportions of the classes in the training and testing data."
      ],
      "metadata": {
        "id": "Y-4FqRSYnUNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "\n",
        "skfolds = StratifiedKFold(n_splits = 3, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
        "  #print(train_index, test_index)\n",
        "  clone_clf = clone(sgd_clf)\n",
        "  X_train_folds = X_train[train_index]\n",
        "  y_train_folds = y_train_5[train_index]  \n",
        "\n",
        "  X_test_fold = X_train[test_index]\n",
        "  y_test_fold = y_train_5[test_index]\n",
        "\n",
        "  clone_clf.fit(X_train_folds, y_train_folds)\n",
        "  y_pred = clone_clf.predict(X_test_fold)\n",
        "  n_correct = sum(y_pred == y_test_fold)\n",
        "  print(n_correct / len(y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc1qsl5pnQAl",
        "outputId": "4a3c3b8b-a40c-4f22-9f9b-779afc0599fd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9669\n",
            "0.91625\n",
            "0.96785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets use the build-in function to evaluate the model \n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JKtinIRn9-b",
        "outputId": "bd3fef0e-36c0-4614-b60e-3a009403d664"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95035, 0.96035, 0.9604 ])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above 93% accuracy is amazing! But before we get too excited, lets look at a very dumb classifier that just classifies every single digit image in the \"not-5\" class:"
      ],
      "metadata": {
        "id": "dH_HJZROyy4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class Never5Classifier(BaseEstimator):\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def predict(self, X):\n",
        "    return np.zeros((len(X),1), dtype=bool)"
      ],
      "metadata": {
        "id": "QLLBI3q5rz59"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try to find out the accuracy of this classifier. "
      ],
      "metadata": {
        "id": "tL5T_ezLzbM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "never_5_clf = Never5Classifier()\n",
        "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring= 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-D9ks_vzZXg",
        "outputId": "8e883f99-f966-449b-8c91-26d4359ddd96"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91125, 0.90855, 0.90915])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time. This shows why accuracy is not a good preferred performance measure for classifiers, especially when you are dealing with skewed datasets.\n",
        "\n",
        "## Why accuracy is not a good performance measure?\n",
        "\n",
        "Accuracy is not always the best performance measure for classifiers because it can be misleading in certain cases. For example, consider a binary classifier that is trying to predict whether a patient has a certain disease or not. If the disease is rare, the classifier could simply predict that every patient does not have the disease, and it would be correct most of the time. However, this classifier would not be very useful because it is not able to detect the rare positive cases.\n",
        "\n",
        "In such cases, it is more informative to look at other performance measures such as precision, recall, and F1 score. Precision measures the proportion of positive predictions that are actually correct, while recall measures the proportion of actual positive cases that were correctly predicted. The F1 score is the harmonic mean of precision and recall, and it is often used as a single metric to evaluate the performance of a classifier.\n",
        "\n",
        "In summary, accuracy is a useful performance measure, but it can be misleading in cases where the classes are imbalanced or the consequences of false predictions are not the same for both classes. In such cases, it is important to consider other performance measures as well.\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table that is used to evaluate the performance of a classifier. It helps to visualize the number of correct and incorrect predictions made by the classifier and allows you to calculate various performance metrics such as precision, recall, and F1 score.\n",
        "\n",
        "The confusion matrix is often used in the field of machine learning and data mining. It is a table with two rows and two columns, and contains the following information:\n",
        "\n",
        "1. **True Positives (TP)**: These are cases where the classifier predicted the positive class, and the true class was also the positive class.\n",
        "2. **True Negatives (TN)**: These are cases where the classifier predicted the negative class, and the true class was also the negative class.\n",
        "3. **False Positives (FP)**: These are cases where the classifier predicted the positive class, but the true class was the negative class. These are also known as \"Type I errors.\"\n",
        "4. **False Negatives (FN)**: These are cases where the classifier predicted the negative class, but the true class was the positive class. These are also known as \"Type II errors.\"\n",
        "\n",
        "Using the values in the confusion matrix, you can calculate various performance metrics such as precision, recall, and F1 score. For example, **precision is calculated as TP / (TP + FP)**, and **recall is calculated as TP / (TP + FN)**. The F1 score is the harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "WYzgHF8X0LdN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s00LlO3zzwSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}